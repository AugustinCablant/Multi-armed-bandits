# Multi-armed-bandits

This repository contains implementations and experiments with Multi-Armed Bandits (MAB) algorithms, including popular strategies such as EXP3 and hierarchical approaches like the NEW algorithm. The goal is to explore bandit problems where each arm represents a decision and rewards are stochastic, with applications in fields like optimization, reinforcement learning, and decision-making.

## Nested bandits
- EXP3 Algorithm: Implements the classic EXP3 (Exponential-weight algorithm for Exploration and Exploitation) for adversarial bandit settings.
- NEW Algorithm: Implements the Nested Weights (NEW) algorithm, a hierarchical bandit method suited for structured decision spaces.
- Comparative Analysis: Includes experiments comparing the performance of EXP3 and NEW in terms of cumulative regret.
- Simulations: Synthetic data is generated to benchmark the algorithms under various reward distributions.


---

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/multi-armed-bandits.git
   cd multi-armed-bandits


## License

This project is licensed under the MIT License. See the LICENSE file for details.


